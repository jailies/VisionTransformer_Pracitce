{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78469.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.11.1\n",
      "  latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.5.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch::pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78485.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.11.1\n",
      "  latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.5.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78499.17s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from pytorch-lightning) (0.11.3.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (69.5.1)\n",
      "Requirement already satisfied: filelock in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from torch>=2.0.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from torch>=2.0.0->pytorch-lightning) (3.1.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10921ebf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78509.80s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/jeonghwan/Desktop/04_pytorch/.conda/lib/python3.12/site-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78549.27s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!chmod u+rwx /Users/jeonghwan/Desktop/vit_practice/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"/Users/jeonghwan/Desktop/vit_practice/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"/Users/jeonghwan/Desktop/vit_practice/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataset, val_dataset = random_split(training_data, [55000, 5000])\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #[batch_size, channel, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131b2c650>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 3, 5, 4, 7, 4, 9, 6, 8, 8, 7, 4, 8, 9, 4, 0, 6, 8, 9, 9, 3, 1, 3, 2,\n",
       "        5, 0, 2, 1, 6, 5, 5, 4, 7, 4, 1, 8, 7, 3, 3, 5, 1, 6, 0, 1, 4, 4, 0, 0,\n",
       "        7, 0, 6, 7, 8, 9, 2, 1, 0, 7, 9, 2, 6, 8, 7, 7, 2, 8, 8, 5, 7, 1, 9, 3,\n",
       "        1, 2, 4, 8, 6, 5, 1, 6, 5, 6, 8, 8, 5, 0, 2, 7, 2, 5, 8, 7, 7, 7, 2, 7,\n",
       "        8, 8, 7, 7, 0, 2, 9, 5, 5, 4, 6, 6, 2, 3, 3, 1, 5, 5, 1, 8, 8, 7, 6, 2,\n",
       "        0, 7, 3, 9, 2, 1, 4, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 7, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Patch Embedding 구현\n",
    "\n",
    "project = nn.Conv2d(1, 16, kernel_size= 4, stride=4)\n",
    "#in_chan = 1\n",
    "#self.emb_size = Channel * Patch Size * Patch Size = 1 * 4 * 4 = 16\n",
    "#Kernel Size = Patch Size \n",
    "#Stride = Patch Size\n",
    "project(data[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS token, Position Embedding 구현\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,in_chan, img_size, patch_size,batch_size=128):\n",
    "        super().__init__()\n",
    "        self.num_patches = int(img_size / pow(patch_size, 2)) # 49\n",
    "        self.emb_size = in_chan * patch_size * patch_size # 16\n",
    "        self.project = nn.Conv2d(in_chan, self.emb_size, kernel_size= patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn(self.num_patches+ 1, self.emb_size)) # [50,16]\n",
    "    \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.project(x)\n",
    "        x = x.view(-1, 49, 16) # [batch_size, 49, 16]\n",
    "        repeat_cls = self.cls_token.repeat(x.size()[0],1,1) #[batch_size, 1 , 16]\n",
    "        x = torch.cat((repeat_cls, x), dim=1)\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.multiheadattention = nn.MultiheadAttention(emb_size, num_heads, batch_first = True, dropout=0.2)\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attn_output, attention = self.multiheadattention(query, key, value)\n",
    "        return attn_output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer, ViT encoder의 MLP 부분\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion = 4, drop_p = 0.2):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,emb_size = 16):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(1, 28*28, 4)\n",
    "        self.Multihead = Multihead(emb_size, 8)\n",
    "        self.FFB = FeedForwardBlock(emb_size)\n",
    "        # Layer Normalization 사용\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        norm_x = self.norm(x)\n",
    "        multihead_output, attention = self.Multihead(norm_x)\n",
    "        \n",
    "        #residual Function\n",
    "        output = multihead_output + x\n",
    "        \n",
    "        norm_output = self.norm(output)\n",
    "        FFB = self.FFB(norm_output)\n",
    "        \n",
    "        final_out = FFB + output\n",
    "        \n",
    "        return final_out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 구현한 모델 여러 번 쌓아서 모델 완성시키기\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_layers: 5, ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([VIT() for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            final_out, attention = layer(x)\n",
    "            \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "ac = torchmetrics.Accuracy(task=\"multiclass\", num_classes = 10).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT_Encoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = []\n",
    "        self.Encoder = nn.Sequential(\n",
    "            TransformerEncoder(n_layers = 5),\n",
    "            #Reduce('b n e -> b e', reduction='mean')\n",
    "        )\n",
    "        self.final_layer = nn.Linear(16, 10)\n",
    "        self.val_loss = []\n",
    "        self.acc = []\n",
    "        self.test_acc =[]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        # 0번째 토큰만 뽑아서 클래스만큼 Layer를 구현\n",
    "        cls_token_final = x[:,0]\n",
    "        #(cls_token_final.shape)\n",
    "        cls_token_final = self.final_layer(cls_token_final)\n",
    "        return cls_token_final\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits,y)\n",
    "\n",
    "        self.loss.append(loss.item())\n",
    "        return loss \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        mean_loss = sum(self.loss) / 430\n",
    "        print(f'traing_loss :{mean_loss}')\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits,y)\n",
    "        self.val_loss.append(loss.item())\n",
    "        acc = ac(logits, y)\n",
    "        self.acc.append(acc)\n",
    "        return loss \n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        mean_loss = sum(self.val_loss) / 40\n",
    "        mean_acc = sum(self.acc)/ 40\n",
    "        print(f'val loss :{mean_loss}, val_acc : {mean_acc}')\n",
    "\n",
    "        self.val_loss = []\n",
    "        self.acc = []\n",
    "        self.log(\"val_loss\", mean_loss)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "        \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        acc = ac(logits, y)\n",
    "        self.test_acc.append(acc)\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        mean_acc = sum(self.test_acc)/ 79\n",
    "        print(mean_acc)\n",
    "\n",
    "\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = torch.backends.mps.is_available()\n",
    "gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', mode ='min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "model = VIT_Encoder()\n",
    "k = model.test_dataloader()\n",
    "b, y = next(iter(k))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | Encoder     | Sequential | 25.8 K | train\n",
      "1 | final_layer | Linear     | 170    | train\n",
      "---------------------------------------------------\n",
      "25.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.9 K    Total params\n",
      "0.104     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 18.50it/s]val loss :0.12786481976509095, val_acc : 0.006640625186264515\n",
      "Epoch 0: 100%|██████████| 430/430 [00:24<00:00, 17.48it/s, v_num=6]        val loss :0.5712673217058182, val_acc : 0.816601574420929\n",
      "Epoch 0: 100%|██████████| 430/430 [00:25<00:00, 16.56it/s, v_num=6]traing_loss :1.3735679455967837\n",
      "Epoch 0: 100%|██████████| 430/430 [00:26<00:00, 16.52it/s, v_num=6]\n"
     ]
    }
   ],
   "source": [
    "model = VIT_Encoder()\n",
    "trainer = pl.Trainer(max_epochs=50, devices=1, callbacks=[early_stop_callback])\n",
    "trainer.fit(model, train_dataloader, val_dataloaders = val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/jeonghwan/Desktop/vit practice/lightning_logs/version_6/checkpoints/epoch=0-step=430.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/jeonghwan/Desktop/vit practice/lightning_logs/version_6/checkpoints/epoch=0-step=430.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 79/79 [00:02<00:00, 32.74it/s]tensor(0.8361, device='mps:0')\n",
      "Testing DataLoader 0: 100%|██████████| 79/79 [00:02<00:00, 32.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
